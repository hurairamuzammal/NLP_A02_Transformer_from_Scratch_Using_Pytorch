{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22F-3853 Muhammad Abu Huraira  \n",
    "# 22F-3410 Shayan Zawar   \n",
    "# NLP A02\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read file and Seperate senetences from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:38.441515Z",
     "iopub.status.busy": "2025-10-20T10:28:38.440951Z",
     "iopub.status.idle": "2025-10-20T10:28:38.571256Z",
     "shell.execute_reply": "2025-10-20T10:28:38.570587Z",
     "shell.execute_reply.started": "2025-10-20T10:28:38.441470Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export only sentences to newer file\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file=pd.read_csv('/kaggle/input/urdu-dataset-20000/final_main_dataset.tsv',sep='\\t')\n",
    "file\n",
    "print(\"Export only sentences to newer file\")\n",
    "sentences = file['sentence']\n",
    "sentences\n",
    "sentences.to_csv('urdu_sentences.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:38.572597Z",
     "iopub.status.busy": "2025-10-20T10:28:38.572257Z",
     "iopub.status.idle": "2025-10-20T10:28:41.732561Z",
     "shell.execute_reply": "2025-10-20T10:28:41.731700Z",
     "shell.execute_reply.started": "2025-10-20T10:28:38.572574Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:41.733949Z",
     "iopub.status.busy": "2025-10-20T10:28:41.733659Z",
     "iopub.status.idle": "2025-10-20T10:28:42.417966Z",
     "shell.execute_reply": "2025-10-20T10:28:42.417219Z",
     "shell.execute_reply.started": "2025-10-20T10:28:41.733916Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting SentencePiece BPE Training Process...\n",
      " Training SentencePiece Tokenizers with BPE...\n",
      "\n",
      " Training Urdu SentencePiece tokenizer...\n",
      " Urdu tokenizer trained: /kaggle/working/urdu_tokenizer.model\n",
      "   Vocabulary file: /kaggle/working/urdu_tokenizer.vocab\n",
      "\n",
      " Testing trained tokenizers...\n",
      "\n",
      " Tokenizer Statistics:\n",
      "Urdu vocabulary size: 10000\n",
      "\n",
      " Test Encoding/Decoding:\n",
      "Original Urdu: ØªÙˆ Ú©Ø¨Ú¾ÛŒ Ø®ÙˆØ¯ Ú©Ùˆ Ø¨Ú¾ÛŒ Ø¯ÛŒÚ©Ú¾Û’ Ú¯Ø§ ØªÙˆ ÚˆØ± Ø¬Ø§Ø¦Û’ Ú¯Ø§\n",
      "Encoded: [68, 397, 490, 36, 76, 6097, 125, 68, 878, 280, 125]\n",
      "Decoded: ØªÙˆ Ú©Ø¨Ú¾ÛŒ Ø®ÙˆØ¯ Ú©Ùˆ Ø¨Ú¾ÛŒ Ø¯ÛŒÚ©Ú¾Û’ Ú¯Ø§ ØªÙˆ ÚˆØ± Ø¬Ø§Ø¦Û’ Ú¯Ø§\n",
      "\n",
      " Tokenizer testing completed!\n",
      "\n",
      " SentencePiece tokenizers with BPE successfully trained!\n",
      " Model files created:\n",
      "   - Urdu: /kaggle/working/urdu_tokenizer.model\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "def train_sentencepiece_tokenizers():    \n",
    "    print(\" Training SentencePiece Tokenizers with BPE...\")\n",
    "    \n",
    "    \n",
    "    urdu_file = \"/kaggle/working/urdu_sentences.txt\"\n",
    "    \n",
    "    urdu_model_prefix = \"/kaggle/working/urdu_tokenizer\"\n",
    "    \n",
    "    # Training parameters for ~10K vocabulary\n",
    "    vocab_size = 10000\n",
    "    \n",
    "    # Train Urdu tokenizer\n",
    "    print(\"\\n Training Urdu SentencePiece tokenizer...\")\n",
    "    if os.path.exists(urdu_file):\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=urdu_file,\n",
    "            model_prefix=urdu_model_prefix,\n",
    "            vocab_size=vocab_size,\n",
    "            model_type='bpe',  # Use BPE algorithm\n",
    "            character_coverage=0.995,  # Good for Urdu script\n",
    "            split_by_unicode_script=True,  # Important for Urdu\n",
    "            split_by_whitespace=True,\n",
    "            normalization_rule_name='nfkc',  # Unicode normalization\n",
    "            remove_extra_whitespaces=True,\n",
    "            max_sentence_length=1000,\n",
    "            shuffle_input_sentence=True,\n",
    "            seed_sentencepiece_size=1000000,\n",
    "            shrinking_factor=0.75,\n",
    "            num_threads=4,\n",
    "            train_extremely_large_corpus=False,\n",
    "            # Special tokens\n",
    "            pad_id=0,\n",
    "            unk_id=1,\n",
    "            bos_id=2,\n",
    "            eos_id=3,\n",
    "            pad_piece='<PAD>',\n",
    "            unk_piece='<UNK>',\n",
    "            bos_piece='<SOS>',\n",
    "            eos_piece='<EOS>',\n",
    "            user_defined_symbols=['<MASK>', '<extra_id_0>', '<extra_id_1>', '<extra_id_2>']\n",
    "        )\n",
    "        print(f\" Urdu tokenizer trained: {urdu_model_prefix}.model\")\n",
    "        print(f\"   Vocabulary file: {urdu_model_prefix}.vocab\")\n",
    "    else:\n",
    "        print(f\" Urdu file not found: {urdu_file}\")\n",
    "    \n",
    "# Test the tokenizers\n",
    "def test_tokenizers(urdu_model_path):\n",
    "    \"\"\"Test the trained tokenizers\"\"\"\n",
    "    print(\"\\n Testing trained tokenizers...\")\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizers\n",
    "        urdu_sp = spm.SentencePieceProcessor()\n",
    "        urdu_sp.load(urdu_model_path)\n",
    "        \n",
    "        # Test sentences\n",
    "        urdu_test = \"ØªÙˆ Ú©Ø¨Ú¾ÛŒ Ø®ÙˆØ¯ Ú©Ùˆ Ø¨Ú¾ÛŒ Ø¯ÛŒÚ©Ú¾Û’ Ú¯Ø§ ØªÙˆ ÚˆØ± Ø¬Ø§Ø¦Û’ Ú¯Ø§\"\n",
    "       \n",
    "        \n",
    "        print(f\"\\n Tokenizer Statistics:\")\n",
    "        print(f\"Urdu vocabulary size: {urdu_sp.get_piece_size()}\")\n",
    "       \n",
    "        \n",
    "        print(f\"\\n Test Encoding/Decoding:\")\n",
    "        print(f\"Original Urdu: {urdu_test}\")\n",
    "        urdu_encoded = urdu_sp.encode(urdu_test, out_type=int)\n",
    "        urdu_decoded = urdu_sp.decode(urdu_encoded)\n",
    "        print(f\"Encoded: {urdu_encoded}\")\n",
    "        print(f\"Decoded: {urdu_decoded}\")\n",
    "        \n",
    "        \n",
    "        print(\"\\n Tokenizer testing completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error testing tokenizers: {e}\")\n",
    "\n",
    "# Train the tokenizers\n",
    "print(\" Starting SentencePiece BPE Training Process...\")\n",
    "urdu_model_path = train_sentencepiece_tokenizers()\n",
    "\n",
    "# Test the trained tokenizers\n",
    "test_tokenizers('/kaggle/working/urdu_tokenizer.model')\n",
    "\n",
    "print(\"\\n SentencePiece tokenizers with BPE successfully trained!\")\n",
    "print(\" Model files created:\")\n",
    "print(f\"   - Urdu: /kaggle/working/urdu_tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.420104Z",
     "iopub.status.busy": "2025-10-20T10:28:42.419714Z",
     "iopub.status.idle": "2025-10-20T10:28:42.438630Z",
     "shell.execute_reply": "2025-10-20T10:28:42.438060Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.420083Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.439453Z",
     "iopub.status.busy": "2025-10-20T10:28:42.439288Z",
     "iopub.status.idle": "2025-10-20T10:28:42.452870Z",
     "shell.execute_reply": "2025-10-20T10:28:42.452177Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.439439Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self,encoder,decoder,src_embed,tgt_embed,generator):\n",
    "        # calling inherited class constructor\n",
    "        super(EncoderDecoder,self).__init__()\n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        self.src_embed=src_embed\n",
    "        self.tgt_embed=tgt_embed\n",
    "        self.generator=generator\n",
    "\n",
    "\n",
    "    # to process src to tgt\n",
    "    def forward(self,src,tgt,src_mask,tgt_mask):\n",
    "        return self.decode(self.encode(src,src_mask),src_mask,tgt,tgt_mask)\n",
    "\n",
    "\n",
    "    def encode(self,src,src_mask):\n",
    "        return self.encoder(self.src_embed(src),src_mask)\n",
    "\n",
    "\n",
    "    def decode(self,memory,src_mask,tgt,tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt),memory,src_mask,tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.453792Z",
     "iopub.status.busy": "2025-10-20T10:28:42.453548Z",
     "iopub.status.idle": "2025-10-20T10:28:42.471062Z",
     "shell.execute_reply": "2025-10-20T10:28:42.470526Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.453767Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# the purpose of this code is to now genrator or predict\n",
    "# word after final layer using softmax\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model,vocab):\n",
    "        super(Generator,self).__init__()\n",
    "        # the linear layer here maps final model vocab to get there probabilities\n",
    "        self.proj=nn.Linear(d_model,vocab)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return F.log_softmax(self.proj(x),dim=1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Decoder Code\n",
    "here we will create encoder decoder code for N=2 layers for encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.471979Z",
     "iopub.status.busy": "2025-10-20T10:28:42.471768Z",
     "iopub.status.idle": "2025-10-20T10:28:42.485168Z",
     "shell.execute_reply": "2025-10-20T10:28:42.484625Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.471955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# code copies modeul N times!\n",
    "def clones(module,N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.486112Z",
     "iopub.status.busy": "2025-10-20T10:28:42.485865Z",
     "iopub.status.idle": "2025-10-20T10:28:42.498082Z",
     "shell.execute_reply": "2025-10-20T10:28:42.497337Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.486085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# entire encoder module consisting of N layers\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,layer,N):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.layers=clones(layer,N)\n",
    "        self.norm=LayerNorm(layer.size)\n",
    "\n",
    "  #\"Here we are passing input and mask through each layer\"\n",
    "    def forward(self,x,mask):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x,mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.499042Z",
     "iopub.status.busy": "2025-10-20T10:28:42.498867Z",
     "iopub.status.idle": "2025-10-20T10:28:42.511439Z",
     "shell.execute_reply": "2025-10-20T10:28:42.510741Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.499026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# add sublayers inputs \n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    # here a is scale and b is shift\n",
    "    def __init__(self,features,eps=1e-6):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.a_2=nn.Parameter(torch.ones(features))\n",
    "        self.b_2=nn.Parameter(torch.zeros(features))\n",
    "        self.eps=eps\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(-1,keepdim=True)\n",
    "        std=x.std(-1,keepdim=True)\n",
    "        return self.a_2*(x-mean)/(std + self.eps)+self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.514191Z",
     "iopub.status.busy": "2025-10-20T10:28:42.514007Z",
     "iopub.status.idle": "2025-10-20T10:28:42.532838Z",
     "shell.execute_reply": "2025-10-20T10:28:42.532273Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.514176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# normalization within each layer\n",
    "class SublayerConnection(nn.Module):\n",
    "\n",
    "    def __init__(self,size,dropout):\n",
    "        super(SublayerConnection,self).__init__()\n",
    "        self.norm=LayerNorm(size)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.533638Z",
     "iopub.status.busy": "2025-10-20T10:28:42.533427Z",
     "iopub.status.idle": "2025-10-20T10:28:42.547656Z",
     "shell.execute_reply": "2025-10-20T10:28:42.547011Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.533623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Each encoder in one layer have two part\n",
    "# ie encoder has attention and feed forward network \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,size,self_attn,feed_forward,dropout):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.self_attn=self_attn\n",
    "        self.feed_forward=feed_forward\n",
    "        self.sublayer=clones(SublayerConnection(size,dropout),2)\n",
    "        self.size=size\n",
    "    def forward(self,x,mask):\n",
    "        x=self.sublayer[0](x,lambda x:self.self_attn(x,x,x,mask))\n",
    "        return self.sublayer[1](x,self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.548542Z",
     "iopub.status.busy": "2025-10-20T10:28:42.548304Z",
     "iopub.status.idle": "2025-10-20T10:28:42.559853Z",
     "shell.execute_reply": "2025-10-20T10:28:42.559308Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.548518Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,layer,N):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.layer=clones(layer,N)\n",
    "        self.norm=LayerNorm(layer.size)\n",
    "    def forward(self,x,memory,src_mask,tgt_mask):\n",
    "        for layer in self.layer:\n",
    "            x=layer(x,memory,src_mask,tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.560915Z",
     "iopub.status.busy": "2025-10-20T10:28:42.560692Z",
     "iopub.status.idle": "2025-10-20T10:28:42.573526Z",
     "shell.execute_reply": "2025-10-20T10:28:42.572863Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.560894Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# unlike encoder that has only attn and feed forward\n",
    "#decoder have two types of attn one for encoders ouput and feed\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,size,self_attn,src_attn,feed_forward,dropout):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.size=size\n",
    "        self.self_attn=self_attn\n",
    "        self.src_attn=src_attn\n",
    "        self.feed_forward=feed_forward\n",
    "        self.sublayer=clones(SublayerConnection(size,dropout),3)\n",
    "        \n",
    "    def forward(self,x,memory,src_mask,tgt_mask):\n",
    "        m=memory\n",
    "        x=self.sublayer[0](x, lambda x:self.self_attn(x,x,x,tgt_mask))\n",
    "        x=self.sublayer[1](x,lambda x:self.src_attn(x,m,m,src_mask))\n",
    "        return self.sublayer[2](x,self.feed_forward)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.574374Z",
     "iopub.status.busy": "2025-10-20T10:28:42.574157Z",
     "iopub.status.idle": "2025-10-20T10:28:42.590913Z",
     "shell.execute_reply": "2025-10-20T10:28:42.590204Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.574352Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# this part of code mask the future \n",
    "# output just like traignle\n",
    "# embedding/tokens to prevent cheating \n",
    "def subsequent_mask(size):\n",
    "    attn_shape=(1,size,size)\n",
    "    subsequent_mask=np.triu(np.ones(attn_shape),k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask)==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.591859Z",
     "iopub.status.busy": "2025-10-20T10:28:42.591584Z",
     "iopub.status.idle": "2025-10-20T10:28:42.604403Z",
     "shell.execute_reply": "2025-10-20T10:28:42.603735Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.591837Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def attention(query,key,value,mask=None,dropout=None):\n",
    "    d_k=query.size(-1)\n",
    "    # to get smooth gradient \n",
    "    scores=torch.matmul(query,key.transpose(-2,-1))/ math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores=scores.masked_fill(mask==0,-1e9)\n",
    "    p_attn=F.softmax(scores,dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn=dropout(p_attn)\n",
    "    return torch.matmul(p_attn,value),p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.605252Z",
     "iopub.status.busy": "2025-10-20T10:28:42.605046Z",
     "iopub.status.idle": "2025-10-20T10:28:42.614182Z",
     "shell.execute_reply": "2025-10-20T10:28:42.613596Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.605236Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention,self).__init__()\n",
    "        assert d_model% h ==0\n",
    "        self.d_k=d_model //h\n",
    "        self.h=h\n",
    "        self.linears=clones(nn.Linear(d_model,d_model),2)\n",
    "        self.attn=None\n",
    "        self.dropout=nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self,query,key,value,mask=None):\n",
    "        if mask is not None:\n",
    "            mask=mask.unsqueeze(1)\n",
    "        nbatches=query.size(0)\n",
    "        query,key,value=[l(x).view(nbatches,-1,self.h,self.d_k).transpose(1,2) for l,x in zip(self.linears,(query,key,value))]\n",
    "        x,self.attn=attention(query,key,value,mask=mask,dropout=self.dropout)\n",
    "    \n",
    "        x=x.transpose(1,2).contiguous().view(nbatches,-1 ,self.h* self.d_k)\n",
    "\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding of Embeddings\n",
    "As Unlike RNN Transformer does not have architecture that allows it to store seq of word to get this behaviour we use positional encoding of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.615002Z",
     "iopub.status.busy": "2025-10-20T10:28:42.614814Z",
     "iopub.status.idle": "2025-10-20T10:28:42.632750Z",
     "shell.execute_reply": "2025-10-20T10:28:42.632133Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.614986Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self,d_model,d_ff,dropout=0.1):\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        self.w_1=nn.Linear(d_model,d_ff)\n",
    "        self.w_2=nn.Linear(d_ff,d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.633783Z",
     "iopub.status.busy": "2025-10-20T10:28:42.633474Z",
     "iopub.status.idle": "2025-10-20T10:28:42.646813Z",
     "shell.execute_reply": "2025-10-20T10:28:42.646111Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.633766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.647784Z",
     "iopub.status.busy": "2025-10-20T10:28:42.647568Z",
     "iopub.status.idle": "2025-10-20T10:28:42.660847Z",
     "shell.execute_reply": "2025-10-20T10:28:42.660142Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.647769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,dropout,max_len=5000):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.dropout=nn.Dropout(p=dropout)\n",
    "\n",
    "        # calculating positional encoding\n",
    "        pe=torch.zeros(max_len,d_model)\n",
    "        position=torch.arange(0,max_len).unsqueeze(1)\n",
    "        div_term=torch.exp(torch.arange(0,d_model,2)* -(math.log(10000.0)/d_model))\n",
    "        pe[:,0::2]=torch.sin(position*div_term)\n",
    "        pe[:,1::2]=torch.cos(position*div_term)\n",
    "        pe=pe.unsqueeze(0)\n",
    "        self.register_buffer('pe',pe)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=x+Variable(self.pe[:,:x.size(1)],requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.877812Z",
     "iopub.status.busy": "2025-10-20T10:28:42.877497Z",
     "iopub.status.idle": "2025-10-20T10:28:42.883325Z",
     "shell.execute_reply": "2025-10-20T10:28:42.882777Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.877782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=2,d_model=512, d_ff=2048, h=4, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches and Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.884366Z",
     "iopub.status.busy": "2025-10-20T10:28:42.884120Z",
     "iopub.status.idle": "2025-10-20T10:28:42.897965Z",
     "shell.execute_reply": "2025-10-20T10:28:42.897355Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.884349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.898960Z",
     "iopub.status.busy": "2025-10-20T10:28:42.898775Z",
     "iopub.status.idle": "2025-10-20T10:28:42.913580Z",
     "shell.execute_reply": "2025-10-20T10:28:42.913020Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.898944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from contextlib import nullcontext\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_epoch(data_loader, model, loss_compute, device, is_train=True, repeats=1, desc=None):\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    base_desc = desc or (\"Training\" if is_train else \"Evaluating\")\n",
    "\n",
    "    for repeat_idx in range(repeats):\n",
    "        pass_desc = base_desc if repeats == 1 else f\"{base_desc} pass {repeat_idx + 1}/{repeats}\"\n",
    "        iterator = tqdm(data_loader, desc=pass_desc, leave=False)\n",
    "        context = nullcontext() if is_train else torch.no_grad()\n",
    "\n",
    "        with context:\n",
    "            for batch_idx, (src, tgt) in enumerate(iterator):\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                batch = Batch(src, tgt, pad=0)\n",
    "\n",
    "                out = model.forward(batch.src, batch.trg,\n",
    "                                    batch.src_mask, batch.trg_mask)\n",
    "                loss_val = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "\n",
    "                logits = model.generator(out)\n",
    "                predictions = logits.argmax(dim=-1)\n",
    "                mask = (batch.trg_y != 0)\n",
    "                correct = ((predictions == batch.trg_y) & mask).sum().item()\n",
    "                \n",
    "                batch_tokens = batch.ntokens.item()\n",
    "                total_loss += loss_val * batch_tokens\n",
    "                total_tokens += batch_tokens\n",
    "                total_correct += correct\n",
    "                \n",
    "                if is_train and batch_idx % 50 == 0:\n",
    "                    batch_acc = (correct / batch_tokens) * 100 if batch_tokens > 0 else 0\n",
    "                    iterator.set_postfix({\n",
    "                        \"loss\": f\"{loss_val:.4f}\",\n",
    "                        \"acc\": f\"{batch_acc:.2f}%\"\n",
    "                    })\n",
    "\n",
    "    avg_loss = (total_loss / total_tokens) if total_tokens else 0.0\n",
    "    avg_accuracy = (total_correct / total_tokens * 100) if total_tokens else 0.0\n",
    "    return avg_loss, avg_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.914516Z",
     "iopub.status.busy": "2025-10-20T10:28:42.914286Z",
     "iopub.status.idle": "2025-10-20T10:28:42.930014Z",
     "shell.execute_reply": "2025-10-20T10:28:42.929313Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.914465Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.931009Z",
     "iopub.status.busy": "2025-10-20T10:28:42.930765Z",
     "iopub.status.idle": "2025-10-20T10:28:42.947513Z",
     "shell.execute_reply": "2025-10-20T10:28:42.946800Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.930986Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.948498Z",
     "iopub.status.busy": "2025-10-20T10:28:42.948266Z",
     "iopub.status.idle": "2025-10-20T10:28:42.960955Z",
     "shell.execute_reply": "2025-10-20T10:28:42.960170Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.948462Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
    "                              y.contiguous().view(-1)) / norm\n",
    "        if self.opt is not None and torch.isfinite(loss):\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.item() if torch.isfinite(loss) else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for Unsupervised Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.961888Z",
     "iopub.status.busy": "2025-10-20T10:28:42.961713Z",
     "iopub.status.idle": "2025-10-20T10:28:42.975206Z",
     "shell.execute_reply": "2025-10-20T10:28:42.974584Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.961873Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "SENTINEL_PIECES = (\"<extra_id_0>\", \"<extra_id_1>\", \"<extra_id_2>\")\n",
    "\n",
    "def get_sentinel_ids(sp, required=2):\n",
    "    ids = []\n",
    "    for piece in SENTINEL_PIECES:\n",
    "        pid = sp.piece_to_id(piece)\n",
    "        if pid != sp.unk_id():\n",
    "            ids.append(pid)\n",
    "    if len(ids) < required:\n",
    "        raise ValueError(\n",
    "            f\"Tokenizer is missing at least {required} sentinel pieces ({SENTINEL_PIECES}); \"\n",
    "            \"regenerate SentencePiece with them via --user_defined_symbols.\"\n",
    "        )\n",
    "    return ids\n",
    "\n",
    "def t5_style_span_corruption(tokens, sentinel_ids, noise_density=0.15, mean_span_length=3.0):\n",
    "    \"\"\"T5-style span corruption.\"\"\"\n",
    "    num_tokens = len(tokens)\n",
    "    num_to_corrupt = int(round(num_tokens * noise_density))\n",
    "    \n",
    "    if num_to_corrupt == 0:\n",
    "        return tokens, [sentinel_ids[0]]\n",
    "\n",
    "    # Calculate the number of spans\n",
    "    num_spans = int(round(num_to_corrupt / mean_span_length))\n",
    "    if num_spans == 0 and num_to_corrupt > 0:\n",
    "        num_spans = 1\n",
    "    \n",
    "    # Select span start positions\n",
    "    span_starts = sorted(random.sample(range(num_tokens), num_spans))\n",
    "    \n",
    "    # Determine span lengths\n",
    "    span_lengths = [int(random.expovariate(1.0 / mean_span_length)) + 1 for _ in range(num_spans)]\n",
    "    \n",
    "    # Create spans\n",
    "    spans = []\n",
    "    used_indices = set()\n",
    "    for start, length in zip(span_starts, span_lengths):\n",
    "        if start in used_indices:\n",
    "            continue\n",
    "        \n",
    "        end = start\n",
    "        current_length = 0\n",
    "        while end < num_tokens and current_length < length:\n",
    "            if end not in used_indices:\n",
    "                used_indices.add(end)\n",
    "                current_length += 1\n",
    "            end += 1\n",
    "        \n",
    "        if current_length > 0:\n",
    "            spans.append((start, end))\n",
    "\n",
    "    spans.sort()\n",
    "\n",
    "    sentinel_cycle = itertools.cycle(sentinel_ids)\n",
    "    \n",
    "    enc_tokens, dec_tokens, cursor = [], [], 0\n",
    "    for start, end in spans:\n",
    "        sentinel = next(sentinel_cycle)\n",
    "        enc_tokens.extend(tokens[cursor:start])\n",
    "        enc_tokens.append(sentinel)\n",
    "        \n",
    "        dec_tokens.append(sentinel)\n",
    "        dec_tokens.extend(tokens[start:end])\n",
    "        cursor = end\n",
    "        \n",
    "    enc_tokens.extend(tokens[cursor:])\n",
    "    return enc_tokens, dec_tokens\n",
    "\n",
    "def replace_with_random_words(tokens, vocab_size, num_to_replace):\n",
    "    #Replace tokens with random tokens from the vocabulary.\n",
    "    if not num_to_replace:\n",
    "        return tokens\n",
    "    \n",
    "    positions = sorted(random.sample(range(len(tokens)), num_to_replace))\n",
    "    \n",
    "    new_tokens = list(tokens)\n",
    "    for pos in positions:\n",
    "        new_tokens[pos] = random.randint(4, vocab_size - 1) # Avoid special tokens\n",
    "        \n",
    "    return new_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.979208Z",
     "iopub.status.busy": "2025-10-20T10:28:42.978742Z",
     "iopub.status.idle": "2025-10-20T10:28:42.995277Z",
     "shell.execute_reply": "2025-10-20T10:28:42.994664Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.979183Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "class UrduDenoiseDataset(Dataset):\n",
    "    def __init__(self, texts, sp, return_original=False):\n",
    "        self.texts = texts\n",
    "        self.sp = sp\n",
    "        self.bos = sp.bos_id()\n",
    "        self.eos = sp.eos_id()\n",
    "        self.sentinel_ids = get_sentinel_ids(sp, required=2)\n",
    "        self.vocab_size = len(sp)\n",
    "        self.return_original = return_original\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        original_text = self.texts[idx]\n",
    "        original_tokens = self.sp.encode(original_text, out_type=int)\n",
    "        \n",
    "        enc_tokens, dec_tokens = t5_style_span_corruption(original_tokens, self.sentinel_ids)\n",
    "\n",
    "        src = torch.tensor([self.bos] + enc_tokens + [self.eos])\n",
    "        tgt = torch.tensor([self.bos] + dec_tokens + [self.eos])\n",
    "        \n",
    "        if self.return_original:\n",
    "            return src, tgt, original_text\n",
    "        return src, tgt\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:42.996246Z",
     "iopub.status.busy": "2025-10-20T10:28:42.995995Z",
     "iopub.status.idle": "2025-10-20T10:28:43.038115Z",
     "shell.execute_reply": "2025-10-20T10:28:43.037521Z",
     "shell.execute_reply.started": "2025-10-20T10:28:42.996229Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: Train=16035, Val=2004, Test=2005\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "def collate_fn(batch):\n",
    "    if len(batch[0]) == 3:  # This is a batch for evaluation\n",
    "        src_batch, tgt_batch, original_texts = zip(*batch)\n",
    "        src_batch = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "        tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
    "        return src_batch, tgt_batch, original_texts\n",
    "    else:  # This is a batch for training\n",
    "        src_batch, tgt_batch = zip(*batch)\n",
    "        src_batch = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "        tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
    "        return src_batch, tgt_batch\n",
    "\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 16\n",
    "\n",
    "with open(\"/kaggle/working/urdu_sentences.txt\", encoding=\"utf-8\") as f:\n",
    "    urdu_texts = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "TOKENIZER_PATH = Path(\"/kaggle/working/urdu_tokenizer.model\")\n",
    "\n",
    "urdu_sp = spm.SentencePieceProcessor()\n",
    "urdu_sp.load(str(TOKENIZER_PATH))\n",
    "\n",
    "\n",
    "train_val_dataset = UrduDenoiseDataset(urdu_texts, urdu_sp, return_original=False)\n",
    "test_dataset_eval = UrduDenoiseDataset(urdu_texts, urdu_sp, return_original=True)\n",
    "\n",
    "\n",
    "# Split dataset: Train 80%, Validation 10%, Test 10%\n",
    "train_size = int(0.8 * len(train_val_dataset))\n",
    "val_size = int(0.1 * len(train_val_dataset))\n",
    "test_size = len(train_val_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, _ = random_split(train_val_dataset, [train_size, val_size, test_size])\n",
    "_, _, test_dataset_eval = random_split(test_dataset_eval, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "print(f\"Dataset split: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset_eval)}\")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset_eval, batch_size=EVAL_BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To use multi gpu for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T10:28:43.039038Z",
     "iopub.status.busy": "2025-10-20T10:28:43.038802Z",
     "iopub.status.idle": "2025-10-20T10:28:43.046378Z",
     "shell.execute_reply": "2025-10-20T10:28:43.045700Z",
     "shell.execute_reply.started": "2025-10-20T10:28:43.039014Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiGPULossCompute:\n",
    "\n",
    "    def __init__(self, generator, criterion, devices, optimizer=None, chunk_size=5):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.devices = devices\n",
    "        self.optimizer = optimizer\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __call__(self, out, targets, normalize=1):\n",
    "        # Scatter to multiple GPUs\n",
    "        out_scatter = nn.parallel.scatter(out, target_gpus=self.devices)\n",
    "        targets_scatter = nn.parallel.scatter(targets, target_gpus=self.devices)\n",
    "\n",
    "        # Replicate modules on each GPU\n",
    "        generator_replicas = nn.parallel.replicate(self.generator, devices=self.devices)\n",
    "        criterion_replicas = nn.parallel.replicate(self.criterion, devices=self.devices)\n",
    "\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Process in chunks to save memory\n",
    "        for i in range(0, out_scatter[0].size(1), self.chunk_size):\n",
    "            out_chunks = [o[:, i:i+self.chunk_size].contiguous() for o in out_scatter]\n",
    "            target_chunks = [t[:, i:i+self.chunk_size].contiguous() for t in targets_scatter]\n",
    "\n",
    "            # Forward pass on each GPU\n",
    "            gen_outputs = nn.parallel.parallel_apply(generator_replicas, out_chunks)\n",
    "\n",
    "            # Compute loss on each GPU\n",
    "            losses = nn.parallel.parallel_apply(criterion_replicas, list(zip(gen_outputs, target_chunks)))\n",
    "\n",
    "            # Combine and backward\n",
    "            loss_tensor = torch.stack([l.mean().to(self.devices[0]) for l in losses])\n",
    "            loss = loss_tensor.mean()\n",
    "            loss.backward()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Optimizer step\n",
    "        if self.optimizer is not None:\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        return total_loss / normalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: T5-Style Span Corruption Training\n",
    "\n",
    "In this phase, we train the model using T5-style span corruption where random spans of text are masked and the model learns to predict the masked content. This approach is excellent for learning robust representations of the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: TRAINING WITH T5-STYLE SPAN CORRUPTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize model for span corruption training\n",
    "model_t5 = make_model(len(urdu_sp), len(urdu_sp), N=2)\n",
    "criterion = LabelSmoothing(size=len(urdu_sp), padding_idx=0, smoothing=0.1)\n",
    "optimizer_t5 = get_std_opt(model_t5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_t5.to(device)\n",
    "\n",
    "def _base_model(module):\n",
    "    return module.module if isinstance(module, nn.DataParallel) else module\n",
    "\n",
    "base_model_t5 = _base_model(model_t5)\n",
    "generator_t5 = base_model_t5.generator\n",
    "\n",
    "best_val_loss_t5 = float('inf')\n",
    "num_epochs_t5 = 15\n",
    "\n",
    "training_history_t5 = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(f\"\\nTraining on device: {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model_t5.parameters()):,}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "for epoch in range(num_epochs_t5):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs_t5}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    loss_compute_train = SimpleLossCompute(generator_t5, criterion, optimizer_t5)\n",
    "    avg_train_loss, avg_train_acc = run_epoch(\n",
    "        train_loader, model_t5, loss_compute_train, device, is_train=True, desc=\"T5 Span Corruption Training\"\n",
    "    )\n",
    "    \n",
    "    loss_compute_eval = SimpleLossCompute(generator_t5, criterion, opt=None)\n",
    "    avg_val_loss, avg_val_acc = run_epoch(\n",
    "        val_loader, model_t5, loss_compute_eval, device, is_train=False, desc=\"Validation\"\n",
    "    )\n",
    "    \n",
    "    # Store history\n",
    "    training_history_t5['train_loss'].append(avg_train_loss)\n",
    "    training_history_t5['train_acc'].append(avg_train_acc)\n",
    "    training_history_t5['val_loss'].append(avg_val_loss)\n",
    "    training_history_t5['val_acc'].append(avg_val_acc)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Epoch {epoch + 1} Results:\")\n",
    "    print(f\"   Train Loss: {avg_train_loss:.4f} | Train Accuracy: {avg_train_acc:.2f}%\")\n",
    "    print(f\"   Val Loss:   {avg_val_loss:.4f} | Val Accuracy:   {avg_val_acc:.2f}%\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss_t5:\n",
    "        best_val_loss_t5 = avg_val_loss\n",
    "        torch.save(base_model_t5.state_dict(), \"urdu_transformer_t5_best.pth\")\n",
    "        print(f\"   âœ“ Best T5 model saved (Val Loss: {best_val_loss_t5:.4f}, Val Acc: {avg_val_acc:.2f}%)\")\n",
    "\n",
    "# Save final T5 model\n",
    "torch.save(base_model_t5.state_dict(), \"urdu_transformer_t5_final.pth\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… T5-Style Span Corruption Training Completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss_t5:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load best model for next phase\n",
    "model_t5.load_state_dict(torch.load(\"urdu_transformer_t5_best.pth\"))\n",
    "model_t5.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Causal Language Modeling Training\n",
    "\n",
    "Now we'll take the pre-trained model from T5-style training and fine-tune it using causal language modeling, where the model learns to predict the next token in a sequence. This is similar to GPT-style training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal Language Modeling Dataset\n",
    "class UrduCausalDataset(Dataset):\n",
    "    \"\"\"Dataset for causal language modeling where input and target are shifted versions.\"\"\"\n",
    "    def __init__(self, texts, sp, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.sp = sp\n",
    "        self.bos = sp.bos_id()\n",
    "        self.eos = sp.eos_id()\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tokens = self.sp.encode(text, out_type=int)\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(tokens) > self.max_length - 2:\n",
    "            tokens = tokens[:self.max_length - 2]\n",
    "        \n",
    "        # Add BOS and EOS\n",
    "        full_tokens = [self.bos] + tokens + [self.eos]\n",
    "        \n",
    "        # For causal LM: input and target are the same sequence\n",
    "        # The model will learn to predict next token\n",
    "        src = torch.tensor(full_tokens)\n",
    "        tgt = torch.tensor(full_tokens)\n",
    "        \n",
    "        return src, tgt\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "# Create causal LM datasets\n",
    "print(\"Creating Causal Language Modeling datasets...\")\n",
    "train_causal_dataset = UrduCausalDataset(urdu_texts, urdu_sp)\n",
    "val_causal_dataset = UrduCausalDataset(urdu_texts, urdu_sp)\n",
    "test_causal_dataset = UrduCausalDataset(urdu_texts, urdu_sp)\n",
    "\n",
    "# Split the data\n",
    "train_size = int(0.8 * len(train_causal_dataset))\n",
    "val_size = int(0.1 * len(train_causal_dataset))\n",
    "test_size = len(train_causal_dataset) - train_size - val_size\n",
    "\n",
    "train_causal_dataset, val_causal_dataset, test_causal_dataset = random_split(\n",
    "    train_causal_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "print(f\"Causal LM Dataset split: Train={len(train_causal_dataset)}, Val={len(val_causal_dataset)}, Test={len(test_causal_dataset)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "def collate_fn_causal(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "train_causal_loader = DataLoader(train_causal_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, collate_fn=collate_fn_causal)\n",
    "val_causal_loader = DataLoader(val_causal_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, collate_fn=collate_fn_causal)\n",
    "test_causal_loader = DataLoader(test_causal_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, collate_fn=collate_fn_causal)\n",
    "\n",
    "print(\"âœ“ Causal LM DataLoaders created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2: FINE-TUNING WITH CAUSAL LANGUAGE MODELING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize model from T5 pretrained weights\n",
    "model_causal = make_model(len(urdu_sp), len(urdu_sp), N=2)\n",
    "print(\"\\nðŸ“¥ Loading T5-pretrained weights...\")\n",
    "model_causal.load_state_dict(torch.load(\"urdu_transformer_t5_best.pth\"))\n",
    "model_causal.to(device)\n",
    "\n",
    "# Create new optimizer for causal training\n",
    "optimizer_causal = get_std_opt(model_causal)\n",
    "\n",
    "base_model_causal = _base_model(model_causal)\n",
    "generator_causal = base_model_causal.generator\n",
    "\n",
    "best_val_loss_causal = float('inf')\n",
    "num_epochs_causal = 10\n",
    "\n",
    "training_history_causal = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ“ Starting causal LM fine-tuning...\")\n",
    "print(f\"Training samples: {len(train_causal_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_causal_dataset)}\")\n",
    "\n",
    "for epoch in range(num_epochs_causal):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs_causal}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    loss_compute_train = SimpleLossCompute(generator_causal, criterion, optimizer_causal)\n",
    "    avg_train_loss, avg_train_acc = run_epoch(\n",
    "        train_causal_loader, model_causal, loss_compute_train, device, is_train=True, desc=\"Causal LM Training\"\n",
    "    )\n",
    "    \n",
    "    loss_compute_eval = SimpleLossCompute(generator_causal, criterion, opt=None)\n",
    "    avg_val_loss, avg_val_acc = run_epoch(\n",
    "        val_causal_loader, model_causal, loss_compute_eval, device, is_train=False, desc=\"Validation\"\n",
    "    )\n",
    "    \n",
    "    # Store history\n",
    "    training_history_causal['train_loss'].append(avg_train_loss)\n",
    "    training_history_causal['train_acc'].append(avg_train_acc)\n",
    "    training_history_causal['val_loss'].append(avg_val_loss)\n",
    "    training_history_causal['val_acc'].append(avg_val_acc)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Epoch {epoch + 1} Results:\")\n",
    "    print(f\"   Train Loss: {avg_train_loss:.4f} | Train Accuracy: {avg_train_acc:.2f}%\")\n",
    "    print(f\"   Val Loss:   {avg_val_loss:.4f} | Val Accuracy:   {avg_val_acc:.2f}%\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss_causal:\n",
    "        best_val_loss_causal = avg_val_loss\n",
    "        torch.save(base_model_causal.state_dict(), \"urdu_transformer_causal_best.pth\")\n",
    "        print(f\"   âœ“ Best Causal model saved (Val Loss: {best_val_loss_causal:.4f}, Val Acc: {avg_val_acc:.2f}%)\")\n",
    "\n",
    "# Save final causal model\n",
    "torch.save(base_model_causal.state_dict(), \"urdu_transformer_causal_final.pth\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Causal Language Modeling Fine-tuning Completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss_causal:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load best model for evaluation\n",
    "model_causal.load_state_dict(torch.load(\"urdu_transformer_causal_best.pth\"))\n",
    "model_causal.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Inference & Evaluation\n",
    "\n",
    "Now we'll evaluate both models (T5 and Causal) with proper inference functions and comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_causal(model, sp, prompt, max_len=50, temperature=0.8, top_k=40, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate text using causal language modeling (like GPT).\n",
    "    The model predicts the next token given previous tokens.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Handle DataParallel\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        actual_model = model.module\n",
    "    else:\n",
    "        actual_model = model\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode the prompt\n",
    "        tokens = sp.encode(prompt, out_type=int)\n",
    "        src = torch.tensor([sp.bos_id()] + tokens, device=device).unsqueeze(0)\n",
    "        \n",
    "        # Create mask for source\n",
    "        src_mask = (src != sp.pad_id()).unsqueeze(-2)\n",
    "        \n",
    "        # Encode the source\n",
    "        memory = actual_model.encode(src, src_mask)\n",
    "        \n",
    "        # Initialize with BOS token\n",
    "        ys = torch.tensor([[sp.bos_id()]], device=device)\n",
    "        generated_ids = []\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            tgt_mask = subsequent_mask(ys.size(1)).to(device)\n",
    "            out = actual_model.decode(memory, src_mask, ys, tgt_mask)\n",
    "            logits = actual_model.generator(out[:, -1])\n",
    "            \n",
    "            # Apply temperature\n",
    "            if temperature != 1.0:\n",
    "                logits = logits / temperature\n",
    "            \n",
    "            # Apply top-k filtering\n",
    "            if top_k > 0:\n",
    "                top_values, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < top_values[:, -1:]] = float('-inf')\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "            \n",
    "            # Stop if EOS token\n",
    "            if next_token == sp.eos_id():\n",
    "                break\n",
    "            \n",
    "            # Avoid special tokens except sentinels\n",
    "            if next_token in [sp.pad_id(), sp.unk_id()]:\n",
    "                continue\n",
    "                \n",
    "            ys = torch.cat([ys, torch.tensor([[next_token]], device=device)], dim=1)\n",
    "            generated_ids.append(next_token)\n",
    "    \n",
    "    # Decode and return\n",
    "    return sp.decode(generated_ids).strip()\n",
    "\n",
    "\n",
    "def generate_text_t5(model, sp, corrupted_input, max_len=100, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate text using T5-style denoising (span corruption recovery).\n",
    "    The model reconstructs the corrupted spans.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Handle DataParallel\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        actual_model = model.module\n",
    "    else:\n",
    "        actual_model = model\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode the corrupted input\n",
    "        if isinstance(corrupted_input, str):\n",
    "            tokens = sp.encode(corrupted_input, out_type=int)\n",
    "            src = torch.tensor([sp.bos_id()] + tokens + [sp.eos_id()], device=device).unsqueeze(0)\n",
    "        else:\n",
    "            src = corrupted_input.to(device)\n",
    "        \n",
    "        src_mask = (src != sp.pad_id()).unsqueeze(-2)\n",
    "        memory = actual_model.encode(src, src_mask)\n",
    "        \n",
    "        # Initialize with BOS token\n",
    "        ys = torch.tensor([[sp.bos_id()]], device=device)\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            tgt_mask = subsequent_mask(ys.size(1)).to(device)\n",
    "            out = actual_model.decode(memory, src_mask, ys, tgt_mask)\n",
    "            prob = actual_model.generator(out[:, -1])\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            next_word = next_word.item()\n",
    "            \n",
    "            if next_word == sp.eos_id():\n",
    "                break\n",
    "                \n",
    "            ys = torch.cat([ys, torch.tensor([[next_word]], device=device)], dim=1)\n",
    "        \n",
    "        # Decode the output\n",
    "        decoded_tokens = ys.cpu().numpy()[0].tolist()\n",
    "        return sp.decode(decoded_tokens).strip()\n",
    "\n",
    "print(\"âœ“ Inference functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def calculate_perplexity(model, dataloader, device):\n",
    "    \"\"\"Calculate perplexity on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0, reduction=\"sum\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Calculating Perplexity\", leave=False):\n",
    "            if len(batch) == 3:\n",
    "                src, tgt, _ = batch\n",
    "            else:\n",
    "                src, tgt = batch\n",
    "            \n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            batch_obj = Batch(src, tgt, pad=0)\n",
    "            \n",
    "            out = model.forward(batch_obj.src, batch_obj.trg, batch_obj.src_mask, batch_obj.trg_mask)\n",
    "            logits = model.generator(out)\n",
    "            loss = criterion(logits.reshape(-1, logits.size(-1)), batch_obj.trg_y.reshape(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_tokens += batch_obj.ntokens.item()\n",
    "            \n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0.0\n",
    "    return math.exp(avg_loss) if avg_loss < 100 else float(\"inf\")\n",
    "\n",
    "\n",
    "def calculate_token_accuracy(pred_tokens, ref_tokens):\n",
    "    \"\"\"Calculate token-level accuracy.\"\"\"\n",
    "    if not ref_tokens:\n",
    "        return 0.0\n",
    "    matches = sum(1 for p, r in zip(pred_tokens, ref_tokens) if p == r)\n",
    "    return matches / len(ref_tokens)\n",
    "\n",
    "\n",
    "def calculate_word_overlap(pred, ref):\n",
    "    \"\"\"Calculate word overlap percentage.\"\"\"\n",
    "    pred_words = set(pred.split())\n",
    "    ref_words = set(ref.split())\n",
    "    if not ref_words:\n",
    "        return 0.0\n",
    "    intersection = pred_words & ref_words\n",
    "    return len(intersection) / len(ref_words)\n",
    "\n",
    "\n",
    "def calculate_char_similarity(pred, ref):\n",
    "    \"\"\"Calculate character-level similarity using sequence matcher.\"\"\"\n",
    "    if not ref:\n",
    "        return 0.0\n",
    "    matcher = SequenceMatcher(None, pred, ref)\n",
    "    return matcher.ratio()\n",
    "\n",
    "\n",
    "def calculate_length_ratio(pred, ref):\n",
    "    \"\"\"Calculate length ratio between prediction and reference.\"\"\"\n",
    "    if not ref:\n",
    "        return 0.0\n",
    "    return min(len(pred), len(ref)) / max(len(pred), len(ref))\n",
    "\n",
    "\n",
    "def safe_bleu_score(predictions, references):\n",
    "    \"\"\"Calculate BLEU score with error handling.\"\"\"\n",
    "    try:\n",
    "        import evaluate\n",
    "        bleu = evaluate.load(\"bleu\")\n",
    "        result = bleu.compute(\n",
    "            predictions=predictions, \n",
    "            references=[[r] for r in references],\n",
    "            smooth=True\n",
    "        )\n",
    "        return result[\"bleu\"] * 100\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def safe_rouge_score(predictions, references):\n",
    "    \"\"\"Calculate ROUGE-L score with error handling.\"\"\"\n",
    "    try:\n",
    "        import evaluate\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        result = rouge.compute(predictions=predictions, references=references)\n",
    "        return result[\"rougeL\"] * 100\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, sp, device, max_samples=None):\n",
    "    \"\"\"Evaluate model and return comprehensive metrics.\"\"\"\n",
    "    model.eval()\n",
    "    predictions, references, inputs = [], [], []\n",
    "    sample_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Generating for Evaluation\", leave=False):\n",
    "            src, _, original_texts = batch\n",
    "            src = src.to(device)\n",
    "            \n",
    "            for i in range(src.size(0)):\n",
    "                if max_samples and sample_count >= max_samples:\n",
    "                    break\n",
    "                \n",
    "                input_text = sp.decode(src[i].cpu().numpy().tolist())\n",
    "                generated_text = generate_text_t5(model, sp, src[i].unsqueeze(0), device=device)\n",
    "                \n",
    "                inputs.append(input_text)\n",
    "                predictions.append(generated_text)\n",
    "                references.append(original_texts[i])\n",
    "                \n",
    "                sample_count += 1\n",
    "            \n",
    "            if max_samples and sample_count >= max_samples:\n",
    "                break\n",
    "    \n",
    "    if not predictions:\n",
    "        print(\"Warning: No valid predictions generated!\")\n",
    "        return None, [], [], []\n",
    "\n",
    "    # Calculate metrics\n",
    "    token_accuracies = []\n",
    "    word_overlaps = []\n",
    "    char_similarities = []\n",
    "    length_ratios = []\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = sp.encode(pred, out_type=int)\n",
    "        ref_tokens = sp.encode(ref, out_type=int)\n",
    "        \n",
    "        token_accuracies.append(calculate_token_accuracy(pred_tokens, ref_tokens))\n",
    "        word_overlaps.append(calculate_word_overlap(pred, ref))\n",
    "        char_similarities.append(calculate_char_similarity(pred, ref))\n",
    "        length_ratios.append(calculate_length_ratio(pred, ref))\n",
    "    \n",
    "    bleu_score = safe_bleu_score(predictions, references)\n",
    "    rouge_score = safe_rouge_score(predictions, references)\n",
    "    \n",
    "    metrics = {\n",
    "        \"bleu\": bleu_score,\n",
    "        \"rougeL\": rouge_score,\n",
    "        \"token_accuracy\": np.mean(token_accuracies) * 100,\n",
    "        \"word_overlap\": np.mean(word_overlaps) * 100,\n",
    "        \"char_similarity\": np.mean(char_similarities) * 100,\n",
    "        \"length_ratio\": np.mean(length_ratios) * 100,\n",
    "    }\n",
    "    \n",
    "    return metrics, predictions, references, inputs\n",
    "\n",
    "print(\"âœ“ Evaluation helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate T5 Model (Span Corruption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING T5 MODEL (SPAN CORRUPTION)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate perplexity for T5 model\n",
    "val_perplexity_t5 = calculate_perplexity(model_t5, val_loader, device)\n",
    "print(f\"\\nðŸ“Š T5 Model Validation Perplexity: {val_perplexity_t5:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "metrics_t5, predictions_t5, references_t5, inputs_t5 = evaluate_model(\n",
    "    model_t5, test_loader, urdu_sp, device, max_samples=100\n",
    ")\n",
    "\n",
    "if metrics_t5:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"T5 MODEL EVALUATION METRICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ Primary Metrics (Custom):\")\n",
    "    print(f\"  â€¢ Token Accuracy:       {metrics_t5['token_accuracy']:.2f}%\")\n",
    "    print(f\"  â€¢ Word Overlap:         {metrics_t5['word_overlap']:.2f}%\")\n",
    "    print(f\"  â€¢ Character Similarity: {metrics_t5['char_similarity']:.2f}%\")\n",
    "    print(f\"  â€¢ Length Ratio:         {metrics_t5['length_ratio']:.2f}%\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š Standard Metrics:\")\n",
    "    print(f\"  â€¢ BLEU Score:           {metrics_t5['bleu']:.4f}\")\n",
    "    print(f\"  â€¢ ROUGE-L Score:        {metrics_t5['rougeL']:.4f}\")\n",
    "    \n",
    "    avg_custom_t5 = (metrics_t5['token_accuracy'] + metrics_t5['word_overlap'] + \n",
    "                     metrics_t5['char_similarity'] + metrics_t5['length_ratio']) / 4\n",
    "    print(f\"\\nâ­ Average Custom Score: {avg_custom_t5:.2f}%\")\n",
    "    \n",
    "    # Show examples\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"T5 MODEL - SAMPLE OUTPUTS\")\n",
    "    print(\"=\"*80)\n",
    "    for idx in range(min(5, len(predictions_t5))):\n",
    "        print(f\"\\nðŸ”¹ Example {idx + 1}\")\n",
    "        print(f\"  Input (Corrupted):  {inputs_t5[idx][:80]}...\")\n",
    "        print(f\"  Expected:           {references_t5[idx][:80]}...\")\n",
    "        print(f\"  Model Output:       {predictions_t5[idx][:80]}...\")\n",
    "        \n",
    "        char_sim = calculate_char_similarity(predictions_t5[idx], references_t5[idx])\n",
    "        print(f\"  Similarity:         {char_sim*100:.1f}%\")\n",
    "\n",
    "print(\"\\nâœ“ T5 Model Evaluation Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Causal LM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING CAUSAL LM MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate perplexity for Causal model\n",
    "val_perplexity_causal = calculate_perplexity(model_causal, val_causal_loader, device)\n",
    "print(f\"\\nðŸ“Š Causal Model Validation Perplexity: {val_perplexity_causal:.4f}\")\n",
    "\n",
    "# Test text generation with Causal model\n",
    "test_prompts = [\n",
    "    \"Ù…ÛŒÚº Ø§Ø³Ú©ÙˆÙ„\",\n",
    "    \"ÛŒÛ Ú©ØªØ§Ø¨\",\n",
    "    \"Ø¢Ø¬ Ù…ÙˆØ³Ù…\",\n",
    "    \"Ù…ÛŒÚº Ù†Û’ Ú©Ú¾Ø§Ù†Ø§\",\n",
    "    \"ÙˆÛ Ú¯Ú¾Ø±\",\n",
    "    \"ØªÙ… Ú©ÛŒØ³Û’\",\n",
    "    \"Ù…ÛŒÚº Ù¹Ú¾ÛŒÚ©\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CAUSAL LM MODEL - TEXT GENERATION EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    try:\n",
    "        generated = generate_text_causal(model_causal, urdu_sp, prompt, max_len=30, temperature=0.7, top_k=40, device=device)\n",
    "        print(f\"\\nðŸ”¹ Example {i}\")\n",
    "        print(f\"  Prompt:      {prompt}\")\n",
    "        print(f\"  Generated:   {prompt} {generated}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nðŸ”¹ Example {i}\")\n",
    "        print(f\"  Prompt:      {prompt}\")\n",
    "        print(f\"  Error:       {str(e)}\")\n",
    "\n",
    "# Calculate metrics on test set\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CALCULATING COMPREHENSIVE METRICS FOR CAUSAL MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# For causal model, we'll measure how well it continues given prompts\n",
    "# We'll take the first few tokens as prompt and compare generation with the rest\n",
    "causal_test_predictions = []\n",
    "causal_test_references = []\n",
    "causal_test_prompts = []\n",
    "\n",
    "model_causal.eval()\n",
    "with torch.no_grad():\n",
    "    sample_count = 0\n",
    "    for batch in tqdm(test_causal_loader, desc=\"Evaluating Causal LM\"):\n",
    "        src, tgt = batch\n",
    "        src = src.to(device)\n",
    "        \n",
    "        for i in range(src.size(0)):\n",
    "            if sample_count >= 100:\n",
    "                break\n",
    "            \n",
    "            # Take first 5 tokens as prompt\n",
    "            prompt_tokens = src[i][:5].cpu().numpy().tolist()\n",
    "            prompt_text = urdu_sp.decode(prompt_tokens)\n",
    "            \n",
    "            # Full text as reference\n",
    "            full_tokens = src[i].cpu().numpy().tolist()\n",
    "            reference_text = urdu_sp.decode(full_tokens)\n",
    "            \n",
    "            # Generate continuation\n",
    "            try:\n",
    "                generated = generate_text_causal(model_causal, urdu_sp, prompt_text, max_len=30, temperature=0.7, device=device)\n",
    "                full_generated = prompt_text + \" \" + generated\n",
    "                \n",
    "                causal_test_prompts.append(prompt_text)\n",
    "                causal_test_predictions.append(full_generated)\n",
    "                causal_test_references.append(reference_text)\n",
    "                \n",
    "                sample_count += 1\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if sample_count >= 100:\n",
    "            break\n",
    "\n",
    "# Calculate metrics\n",
    "if causal_test_predictions:\n",
    "    token_accuracies = []\n",
    "    word_overlaps = []\n",
    "    char_similarities = []\n",
    "    length_ratios = []\n",
    "    \n",
    "    for pred, ref in zip(causal_test_predictions, causal_test_references):\n",
    "        pred_tokens = urdu_sp.encode(pred, out_type=int)\n",
    "        ref_tokens = urdu_sp.encode(ref, out_type=int)\n",
    "        \n",
    "        token_accuracies.append(calculate_token_accuracy(pred_tokens, ref_tokens))\n",
    "        word_overlaps.append(calculate_word_overlap(pred, ref))\n",
    "        char_similarities.append(calculate_char_similarity(pred, ref))\n",
    "        length_ratios.append(calculate_length_ratio(pred, ref))\n",
    "    \n",
    "    bleu_score_causal = safe_bleu_score(causal_test_predictions, causal_test_references)\n",
    "    rouge_score_causal = safe_rouge_score(causal_test_predictions, causal_test_references)\n",
    "    \n",
    "    metrics_causal = {\n",
    "        \"bleu\": bleu_score_causal,\n",
    "        \"rougeL\": rouge_score_causal,\n",
    "        \"token_accuracy\": np.mean(token_accuracies) * 100,\n",
    "        \"word_overlap\": np.mean(word_overlaps) * 100,\n",
    "        \"char_similarity\": np.mean(char_similarities) * 100,\n",
    "        \"length_ratio\": np.mean(length_ratios) * 100,\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CAUSAL LM MODEL EVALUATION METRICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ Primary Metrics (Custom):\")\n",
    "    print(f\"  â€¢ Token Accuracy:       {metrics_causal['token_accuracy']:.2f}%\")\n",
    "    print(f\"  â€¢ Word Overlap:         {metrics_causal['word_overlap']:.2f}%\")\n",
    "    print(f\"  â€¢ Character Similarity: {metrics_causal['char_similarity']:.2f}%\")\n",
    "    print(f\"  â€¢ Length Ratio:         {metrics_causal['length_ratio']:.2f}%\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š Standard Metrics:\")\n",
    "    print(f\"  â€¢ BLEU Score:           {metrics_causal['bleu']:.4f}\")\n",
    "    print(f\"  â€¢ ROUGE-L Score:        {metrics_causal['rougeL']:.4f}\")\n",
    "    \n",
    "    avg_custom_causal = (metrics_causal['token_accuracy'] + metrics_causal['word_overlap'] + \n",
    "                         metrics_causal['char_similarity'] + metrics_causal['length_ratio']) / 4\n",
    "    print(f\"\\nâ­ Average Custom Score: {avg_custom_causal:.2f}%\")\n",
    "    \n",
    "    # Show examples\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CAUSAL LM - SAMPLE CONTINUATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    for idx in range(min(5, len(causal_test_predictions))):\n",
    "        print(f\"\\nðŸ”¹ Example {idx + 1}\")\n",
    "        print(f\"  Prompt:      {causal_test_prompts[idx]}\")\n",
    "        print(f\"  Reference:   {causal_test_references[idx][:80]}...\")\n",
    "        print(f\"  Generated:   {causal_test_predictions[idx][:80]}...\")\n",
    "        \n",
    "        char_sim = calculate_char_similarity(causal_test_predictions[idx], causal_test_references[idx])\n",
    "        print(f\"  Similarity:  {char_sim*100:.1f}%\")\n",
    "\n",
    "print(\"\\nâœ“ Causal LM Evaluation Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Packaging & Export\n",
    "\n",
    "Package all models, tokenizer, configurations, and training history into a complete zip file for deployment and sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "# Create export directory\n",
    "export_dir = Path(\"/kaggle/working/model_export\")\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PACKAGING MODELS FOR EXPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Save Model Configurations\n",
    "print(\"\\nðŸ“ Saving model configurations...\")\n",
    "\n",
    "model_configs = {\n",
    "    \"model_architecture\": {\n",
    "        \"src_vocab\": len(urdu_sp),\n",
    "        \"tgt_vocab\": len(urdu_sp),\n",
    "        \"N\": 2,\n",
    "        \"d_model\": 512,\n",
    "        \"d_ff\": 2048,\n",
    "        \"h\": 8,\n",
    "        \"dropout\": 0.1,\n",
    "    },\n",
    "    \"training_info\": {\n",
    "        \"t5_epochs\": num_epochs_t5,\n",
    "        \"t5_best_val_loss\": best_val_loss_t5,\n",
    "        \"causal_epochs\": num_epochs_causal,\n",
    "        \"causal_best_val_loss\": best_val_loss_causal,\n",
    "        \"tokenizer_vocab_size\": len(urdu_sp),\n",
    "        \"dataset_size\": len(urdu_texts),\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"val_size\": len(val_dataset),\n",
    "        \"test_size\": len(test_dataset_eval),\n",
    "    },\n",
    "    \"evaluation_metrics\": {\n",
    "        \"t5_model\": metrics_t5 if metrics_t5 else {},\n",
    "        \"causal_model\": metrics_causal if 'metrics_causal' in locals() else {},\n",
    "        \"t5_perplexity\": val_perplexity_t5,\n",
    "        \"causal_perplexity\": val_perplexity_causal if 'val_perplexity_causal' in locals() else None,\n",
    "    },\n",
    "    \"export_info\": {\n",
    "        \"export_date\": datetime.now().isoformat(),\n",
    "        \"framework\": \"PyTorch\",\n",
    "        \"python_version\": \"3.10+\",\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = export_dir / \"model_config.json\"\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_configs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ“ Configuration saved to {config_path}\")\n",
    "\n",
    "# 2. Save Training History\n",
    "print(\"\\nðŸ“Š Saving training history...\")\n",
    "\n",
    "training_history = {\n",
    "    \"t5_training\": training_history_t5,\n",
    "    \"causal_training\": training_history_causal,\n",
    "}\n",
    "\n",
    "history_path = export_dir / \"training_history.json\"\n",
    "with open(history_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Training history saved to {history_path}\")\n",
    "\n",
    "# 3. Save Model Bundles\n",
    "print(\"\\nðŸ’¾ Saving model bundles...\")\n",
    "\n",
    "# T5 Model Bundle\n",
    "t5_bundle = {\n",
    "    \"state_dict\": {k: v.cpu() for k, v in base_model_t5.state_dict().items()},\n",
    "    \"config\": model_configs[\"model_architecture\"],\n",
    "    \"training_type\": \"t5_span_corruption\",\n",
    "    \"best_val_loss\": best_val_loss_t5,\n",
    "    \"exported_at\": datetime.now().isoformat(),\n",
    "}\n",
    "t5_bundle_path = export_dir / \"urdu_transformer_t5.pt\"\n",
    "torch.save(t5_bundle, t5_bundle_path)\n",
    "print(f\"âœ“ T5 model bundle saved to {t5_bundle_path}\")\n",
    "\n",
    "# Causal Model Bundle\n",
    "causal_bundle = {\n",
    "    \"state_dict\": {k: v.cpu() for k, v in base_model_causal.state_dict().items()},\n",
    "    \"config\": model_configs[\"model_architecture\"],\n",
    "    \"training_type\": \"causal_language_modeling\",\n",
    "    \"best_val_loss\": best_val_loss_causal,\n",
    "    \"pretrained_from\": \"t5_span_corruption\",\n",
    "    \"exported_at\": datetime.now().isoformat(),\n",
    "}\n",
    "causal_bundle_path = export_dir / \"urdu_transformer_causal.pt\"\n",
    "torch.save(causal_bundle, causal_bundle_path)\n",
    "print(f\"âœ“ Causal model bundle saved to {causal_bundle_path}\")\n",
    "\n",
    "# 4. Copy Tokenizer Files\n",
    "print(\"\\nðŸ”¤ Copying tokenizer files...\")\n",
    "\n",
    "tokenizer_files = [\n",
    "    \"/kaggle/working/urdu_tokenizer.model\",\n",
    "    \"/kaggle/working/urdu_tokenizer.vocab\"\n",
    "]\n",
    "\n",
    "for tok_file in tokenizer_files:\n",
    "    tok_path = Path(tok_file)\n",
    "    if tok_path.exists():\n",
    "        shutil.copy(tok_path, export_dir / tok_path.name)\n",
    "        print(f\"âœ“ Copied {tok_path.name}\")\n",
    "\n",
    "# 5. Create README\n",
    "print(\"\\nðŸ“„ Creating README...\")\n",
    "\n",
    "# Prepare conditional values safely\n",
    "try:\n",
    "    causal_perp_value = val_perplexity_causal\n",
    "    causal_perp_str = f\"{causal_perp_value:.4f}\"\n",
    "except (NameError, UnboundLocalError):\n",
    "    causal_perp_str = 'N/A'\n",
    "\n",
    "try:\n",
    "    causal_score_value = avg_custom_causal\n",
    "    causal_score_str = f\"{causal_score_value:.2f}%\"\n",
    "except (NameError, UnboundLocalError):\n",
    "    causal_score_str = 'N/A'\n",
    "\n",
    "readme_content = f\"\"\"# Urdu Transformer Models - Export Package\n",
    "\n",
    "**Export Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Package Contents\n",
    "\n",
    "This package contains two trained transformer models for Urdu language processing:\n",
    "\n",
    "### 1. T5-Style Span Corruption Model\n",
    "- **File:** `urdu_transformer_t5.pt`\n",
    "- **Training Method:** T5-style span corruption (denoising)\n",
    "- **Best Validation Loss:** {best_val_loss_t5:.4f}\n",
    "- **Epochs Trained:** {num_epochs_t5}\n",
    "\n",
    "### 2. Causal Language Model\n",
    "- **File:** `urdu_transformer_causal.pt`\n",
    "- **Training Method:** Causal language modeling (GPT-style)\n",
    "- **Best Validation Loss:** {best_val_loss_causal:.4f}\n",
    "- **Epochs Trained:** {num_epochs_causal}\n",
    "- **Pre-trained From:** T5 model\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "- **Layers (N):** 2 encoder + 2 decoder layers\n",
    "- **Model Dimension (d_model):** 512\n",
    "- **Feed-forward Dimension (d_ff):** 2048\n",
    "- **Attention Heads (h):** 8\n",
    "- **Dropout:** 0.1\n",
    "- **Vocabulary Size:** {len(urdu_sp)}\n",
    "- **Total Parameters:** ~{sum(p.numel() for p in model_t5.parameters()):,}\n",
    "\n",
    "## Tokenizer\n",
    "\n",
    "- **Type:** SentencePiece BPE\n",
    "- **Vocabulary Size:** {len(urdu_sp)}\n",
    "- **Files:** \n",
    "  - `urdu_tokenizer.model` (main model)\n",
    "  - `urdu_tokenizer.vocab` (vocabulary)\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "- **Total Sentences:** {len(urdu_texts):,}\n",
    "- **Training Split:** {len(train_dataset):,} samples (80%)\n",
    "- **Validation Split:** {len(val_dataset):,} samples (10%)\n",
    "- **Test Split:** {len(test_dataset_eval):,} samples (10%)\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "### T5 Model\n",
    "- **Perplexity:** {val_perplexity_t5:.4f}\n",
    "- **Custom Score:** {avg_custom_t5:.2f}%\n",
    "\n",
    "### Causal Model\n",
    "- **Perplexity:** {causal_perp_str}\n",
    "- **Custom Score:** {causal_score_str}\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Loading the Models\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Load tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('urdu_tokenizer.model')\n",
    "\n",
    "# Load T5 model\n",
    "t5_checkpoint = torch.load('urdu_transformer_t5.pt', map_location='cpu')\n",
    "# ... create model with config and load state_dict\n",
    "\n",
    "# Load Causal model\n",
    "causal_checkpoint = torch.load('urdu_transformer_causal.pt', map_location='cpu')\n",
    "# ... create model with config and load state_dict\n",
    "```\n",
    "\n",
    "### Model Selection Guide\n",
    "\n",
    "- **Use T5 Model for:**\n",
    "  - Text denoising and correction\n",
    "  - Filling in masked/corrupted text\n",
    "  - Sequence-to-sequence tasks\n",
    "\n",
    "- **Use Causal Model for:**\n",
    "  - Text generation and continuation\n",
    "  - Autocomplete functionality\n",
    "  - Creative writing assistance\n",
    "\n",
    "## Files Included\n",
    "\n",
    "1. `urdu_transformer_t5.pt` - T5 model bundle\n",
    "2. `urdu_transformer_causal.pt` - Causal LM model bundle\n",
    "3. `urdu_tokenizer.model` - SentencePiece tokenizer\n",
    "4. `urdu_tokenizer.vocab` - Tokenizer vocabulary\n",
    "5. `model_config.json` - Complete configuration and metrics\n",
    "6. `training_history.json` - Training progress history\n",
    "7. `README.md` - This file\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Python 3.10+\n",
    "- PyTorch 2.0+\n",
    "- SentencePiece\n",
    "- NumPy\n",
    "\n",
    "## Authors\n",
    "\n",
    "- Muhammad Abu Huraira (22F-3853)\n",
    "- Shayan Zawar (22F-3410)\n",
    "\n",
    "## License\n",
    "\n",
    "See LICENSE file for details.\n",
    "\n",
    "---\n",
    "\n",
    "Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "readme_path = export_dir / \"README.md\"\n",
    "with open(readme_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(f\"âœ“ README saved to {readme_path}\")\n",
    "\n",
    "print(\"\\nâœ… All files prepared for export!\")\n",
    "print(f\"\\nðŸ“ Export directory: {export_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final ZIP file\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING ZIP ARCHIVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "zip_filename = \"/kaggle/working/urdu_transformer_complete_export.zip\"\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add all files from export directory\n",
    "    for file_path in export_dir.rglob('*'):\n",
    "        if file_path.is_file():\n",
    "            arcname = file_path.relative_to(export_dir.parent)\n",
    "            zipf.write(file_path, arcname)\n",
    "            print(f\"  âœ“ Added: {arcname}\")\n",
    "\n",
    "# Get file size\n",
    "zip_size = Path(zip_filename).stat().st_size / (1024 * 1024)  # Size in MB\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… EXPORT COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ“¦ ZIP File Created: {zip_filename}\")\n",
    "print(f\"ðŸ“Š File Size: {zip_size:.2f} MB\")\n",
    "print(f\"\\nðŸ“ Package Contents:\")\n",
    "print(f\"   â€¢ 2 Trained Models (T5 + Causal)\")\n",
    "print(f\"   â€¢ Tokenizer (SentencePiece)\")\n",
    "print(f\"   â€¢ Complete Configurations\")\n",
    "print(f\"   â€¢ Training History\")\n",
    "print(f\"   â€¢ Evaluation Metrics\")\n",
    "print(f\"   â€¢ README Documentation\")\n",
    "print(\"\\nðŸŽ‰ Ready for download and deployment!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary & Comparison\n",
    "\n",
    "Compare the performance of both training approaches and summarize the complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL SUMMARY & COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison table\n",
    "print(\"\\nðŸ“Š MODEL COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Metric':<30} {'T5 Model':<20} {'Causal Model':<20}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Training info\n",
    "print(f\"{'Training Approach':<30} {'Span Corruption':<20} {'Causal LM':<20}\")\n",
    "print(f\"{'Epochs Trained':<30} {num_epochs_t5:<20} {num_epochs_causal:<20}\")\n",
    "print(f\"{'Best Val Loss':<30} {best_val_loss_t5:<20.4f} {best_val_loss_causal:<20.4f}\")\n",
    "print(f\"{'Validation Perplexity':<30} {val_perplexity_t5:<20.4f} {val_perplexity_causal if 'val_perplexity_causal' in locals() else 0:<20.4f}\")\n",
    "\n",
    "# Metrics comparison\n",
    "if metrics_t5 and 'metrics_causal' in locals():\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'Token Accuracy (%)':<30} {metrics_t5['token_accuracy']:<20.2f} {metrics_causal['token_accuracy']:<20.2f}\")\n",
    "    print(f\"{'Word Overlap (%)':<30} {metrics_t5['word_overlap']:<20.2f} {metrics_causal['word_overlap']:<20.2f}\")\n",
    "    print(f\"{'Char Similarity (%)':<30} {metrics_t5['char_similarity']:<20.2f} {metrics_causal['char_similarity']:<20.2f}\")\n",
    "    print(f\"{'Length Ratio (%)':<30} {metrics_t5['length_ratio']:<20.2f} {metrics_causal['length_ratio']:<20.2f}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'BLEU Score':<30} {metrics_t5['bleu']:<20.4f} {metrics_causal['bleu']:<20.4f}\")\n",
    "    print(f\"{'ROUGE-L Score':<30} {metrics_t5['rougeL']:<20.4f} {metrics_causal['rougeL']:<20.4f}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'Avg Custom Score (%)':<30} {avg_custom_t5:<20.2f} {avg_custom_causal:<20.2f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Plot training history\n",
    "print(\"\\nðŸ“ˆ Plotting training history...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Training History Comparison: T5 vs Causal LM', fontsize=16, fontweight='bold')\n",
    "\n",
    "# T5 Loss\n",
    "axes[0, 0].plot(training_history_t5['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0, 0].plot(training_history_t5['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0, 0].set_title('T5 Model - Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# T5 Accuracy\n",
    "axes[0, 1].plot(training_history_t5['train_acc'], label='Train Acc', marker='o', color='green')\n",
    "axes[0, 1].plot(training_history_t5['val_acc'], label='Val Acc', marker='s', color='orange')\n",
    "axes[0, 1].set_title('T5 Model - Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Causal Loss\n",
    "axes[1, 0].plot(training_history_causal['train_loss'], label='Train Loss', marker='o')\n",
    "axes[1, 0].plot(training_history_causal['val_loss'], label='Val Loss', marker='s')\n",
    "axes[1, 0].set_title('Causal LM - Loss')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Causal Accuracy\n",
    "axes[1, 1].plot(training_history_causal['train_acc'], label='Train Acc', marker='o', color='green')\n",
    "axes[1, 1].plot(training_history_causal['val_acc'], label='Val Acc', marker='s', color='orange')\n",
    "axes[1, 1].set_title('Causal LM - Accuracy')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy (%)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/training_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Training history plot saved!\")\n",
    "\n",
    "# Create metrics comparison bar chart\n",
    "if metrics_t5 and 'metrics_causal' in locals():\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    \n",
    "    metrics_names = ['Token Accuracy', 'Word Overlap', 'Char Similarity', 'Length Ratio']\n",
    "    t5_scores = [\n",
    "        metrics_t5['token_accuracy'],\n",
    "        metrics_t5['word_overlap'],\n",
    "        metrics_t5['char_similarity'],\n",
    "        metrics_t5['length_ratio']\n",
    "    ]\n",
    "    causal_scores = [\n",
    "        metrics_causal['token_accuracy'],\n",
    "        metrics_causal['word_overlap'],\n",
    "        metrics_causal['char_similarity'],\n",
    "        metrics_causal['length_ratio']\n",
    "    ]\n",
    "    \n",
    "    x = range(len(metrics_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar([i - width/2 for i in x], t5_scores, width, label='T5 Model', color='skyblue')\n",
    "    bars2 = ax.bar([i + width/2 for i in x], causal_scores, width, label='Causal LM', color='lightcoral')\n",
    "    \n",
    "    ax.set_xlabel('Metrics', fontweight='bold')\n",
    "    ax.set_ylabel('Score (%)', fontweight='bold')\n",
    "    ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics_names)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/kaggle/working/metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ“ Metrics comparison plot saved!\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“‹ COMPLETE PIPELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "âœ… Pipeline Completed Successfully!\n",
    "\n",
    "Phase 1: T5-Style Span Corruption Training\n",
    "  - Trained transformer with denoising objective\n",
    "  - Model learns to reconstruct corrupted text spans\n",
    "  - Best for text correction and infilling tasks\n",
    "\n",
    "Phase 2: Causal Language Modeling\n",
    "  - Fine-tuned T5 model with causal LM objective\n",
    "  - Model learns to predict next tokens\n",
    "  - Best for text generation and completion\n",
    "\n",
    "Deliverables:\n",
    "  âœ“ Two fully trained models (T5 + Causal)\n",
    "  âœ“ SentencePiece tokenizer with 10K vocab\n",
    "  âœ“ Complete configuration files\n",
    "  âœ“ Training history and metrics\n",
    "  âœ“ Comprehensive evaluation results\n",
    "  âœ“ README documentation\n",
    "  âœ“ Complete ZIP package for deployment\n",
    "\n",
    "All files are packaged and ready for download!\n",
    "\"\"\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3640718,
     "sourceId": 6329548,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
